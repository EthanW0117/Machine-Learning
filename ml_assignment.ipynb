{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python [conda env:mie451-assignment-ml]",
      "language": "python",
      "name": "conda-env-mie451-assignment-ml-py"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.6"
    },
    "colab": {
      "name": "ml_assignment.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "B3NEe4BojNQs"
      },
      "source": [
        "!pip install wget"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CrWhZK_sjJWd"
      },
      "source": [
        "import os\n",
        "import re\n",
        "from pathlib import Path\n",
        "from nltk.tokenize import RegexpTokenizer\n",
        "from collections import Counter\n",
        "import pandas as pd\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.naive_bayes import GaussianNB, BernoulliNB, MultinomialNB\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix\n",
        "from sklearn.exceptions import ConvergenceWarning\n",
        "import numpy as np\n",
        "from scipy import stats\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline  \n",
        "import random\n",
        "import wget\n",
        "random.seed()\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
        "warnings.filterwarnings(\"ignore\", category=ConvergenceWarning)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ccMNbLzljJWl"
      },
      "source": [
        "filename = wget.download(\"https://github.com/MIE451-1513-2019/course-datasets/raw/master/20_newsgroups.zip\", \"20_newsgroups.zip\")\n",
        "!unzip 20_newsgroups.zip\n",
        "DATA_DIR = \"20_newsgroups\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZAOneB4BjJWq"
      },
      "source": [
        "## Functions from lab"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "isVIWKz5jJWr"
      },
      "source": [
        "def clean_file_text(text):\n",
        "    new_text = re.sub(\"Newsgroups:.*?\\n\", \"\", text)\n",
        "    new_text = re.sub(\"Xref:.*?\\n\", \"\", new_text)\n",
        "    new_text = re.sub(\"Path:.*?\\n\", \"\", new_text)\n",
        "    new_text = re.sub(\"Date:.*?\\n\", \"\", new_text)\n",
        "    new_text = re.sub(\"Followup-To:.*?\\n\", \"\", new_text)\n",
        "    return new_text"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0HHTRr8CjJWv"
      },
      "source": [
        "def corpus_count_words(file_list):\n",
        "    tokenizer = RegexpTokenizer(r'\\w+')\n",
        "    word_counter = Counter()\n",
        "    for file_path in file_list:\n",
        "        with open(file_path, 'r', encoding='utf-8', errors='ignore') as file:\n",
        "            file_data = file.read()\n",
        "            file_data = clean_file_text(file_data)\n",
        "            file_words = tokenizer.tokenize(file_data)\n",
        "            word_counter.update(file_words)\n",
        "    return word_counter"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_VBp_7RBjJW1"
      },
      "source": [
        "def get_topic_name(file_path):\n",
        "    return file_path.parent.name\n",
        "\n",
        "def get_target(topic_name):\n",
        "    topics = ['talk.politics.mideast', 'rec.autos', 'comp.sys.mac.hardware', 'alt.atheism', 'rec.sport.baseball', \n",
        "     'comp.os.ms-windows.misc', 'rec.sport.hockey', 'sci.crypt', 'sci.med', 'talk.politics.misc', \n",
        "     'rec.motorcycles', 'comp.windows.x', 'comp.graphics', 'comp.sys.ibm.pc.hardware', 'sci.electronics',\n",
        "     'talk.politics.guns', 'sci.space', 'soc.religion.christian', 'misc.forsale', 'talk.religion.misc']\n",
        "    return topics.index(topic_name)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OH81rhWjjJW4"
      },
      "source": [
        "def plot_confusion_matrix(cm):\n",
        "    # plot the confusion matrix\n",
        "    plt.figure(figsize=(10,10))\n",
        "    plt.matshow(cm, fignum=1)\n",
        "    \n",
        "    # add labels for all targets\n",
        "    num_targets = cm.shape[0]\n",
        "    plt.xticks(list(range(num_targets+1)))\n",
        "    plt.yticks(list(range(num_targets+1)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0TuPzSUPjJW8"
      },
      "source": [
        "## Q1"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QwVZblX1jJW9"
      },
      "source": [
        "all_files = [pth for pth in Path(DATA_DIR).glob(\"**/*\") if pth.is_file() and not pth.name.startswith(\".\")]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jn3bD8tCjJXA"
      },
      "source": [
        "def binary_baseline_data(file_list, num_words = 1000):\n",
        "    # Calculate word count in corpus\n",
        "    news_cnt = corpus_count_words(file_list)\n",
        "    \n",
        "    # Select the most common numWords\n",
        "    word_list = [word for (word, freq) in news_cnt.most_common(num_words)]\n",
        "    \n",
        "    # Create a binary encoding of dataset based on the selected features (X)\n",
        "    tokenizer = RegexpTokenizer(r'\\w+')\n",
        "    df_rows = []\n",
        "    for file_path in file_list:\n",
        "        with open(file_path, 'r', encoding='utf-8', errors='ignore') as file:\n",
        "            file_data = file.read()\n",
        "            file_data = clean_file_text(file_data)\n",
        "            file_words = tokenizer.tokenize(file_data)\n",
        "            df_rows.append([1 if word in file_words else 0 for word in word_list])      \n",
        "    X = pd.DataFrame(df_rows, index=[str(f) for f in file_list], columns = word_list)\n",
        "    \n",
        "    # Create a dataframe of targets (y)\n",
        "    y = [get_target(get_topic_name(file_path)) for file_path in file_list]\n",
        "    \n",
        "    return X, y"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hV2I4MIDjJXF"
      },
      "source": [
        "# get the baseline data\n",
        "X, y = binary_baseline_data(all_files)\n",
        "\n",
        "# split to train and test set\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# train a logistic regression classifier\n",
        "clf = LogisticRegression(C=1.0).fit(X_train, y_train)\n",
        "\n",
        "# predict on train and test set\n",
        "y_train_predict = clf.predict(X_train)\n",
        "y_test_predict = clf.predict(X_test)\n",
        "\n",
        "# calculate train and test accuracy\n",
        "train_accuracy = accuracy_score(y_train, y_train_predict)\n",
        "test_accuracy = accuracy_score(y_test, y_test_predict)\n",
        "\n",
        "# report results\n",
        "print(\"Train accuracy: {}\".format(train_accuracy))\n",
        "print(\"Test accuracy: {}\".format(test_accuracy))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l_ka2WqV-B6J"
      },
      "source": [
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dZSQINnXjJXJ"
      },
      "source": [
        "### Q1 (a)\n",
        "\n",
        "[Write your answer here]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3XFlNTqFjJXL"
      },
      "source": [
        "### Q1 (b)\n",
        "\n",
        "Modify the following function:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6GM_i9r9jJXM"
      },
      "source": [
        "def binary_improved_data(file_list, num_words = 1000):\n",
        "    # Put your code here\n",
        "    # Make sure you update the variable features and targets below\n",
        "    \n",
        "    X = None\n",
        "    y = None\n",
        "\n",
        "    #Please remember to put index for your dataframe as the file name\n",
        "    #For example: pd.DataFrame(data, index=[str(f) for f in file_list],columns=[])\n",
        "    \n",
        "    # validate return types\n",
        "    assert isinstance(X, pd.DataFrame) and isinstance(y, list), \"return types\"\n",
        "    \n",
        "    return X, y"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LXRUw2LDjJXQ"
      },
      "source": [
        "### Q1 (c)\n",
        "\n",
        "Modify the following partial code to calculate the train and test accuracy and answer the question in the markdown cell below"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G2wliJNvjJXR"
      },
      "source": [
        "# get the baseline data\n",
        "X, y = binary_improved_data(all_files)\n",
        "\n",
        "# Write your code here:\n",
        "# You need to split the data and train a logistic regression classifier.\n",
        "# Then, you need to calculate the variables train_accuracy and test_accuracy for the new classifier\n",
        "\n",
        "\n",
        "\n",
        "# report results\n",
        "print(\"Train accuracy: {}\".format(train_accuracy))\n",
        "print(\"Test accuracy: {}\".format(test_accuracy))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SST3B968jJXX"
      },
      "source": [
        "[Write your answer here]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K2EbQVhMjJXY"
      },
      "source": [
        "### Q1 (d)\n",
        "\n",
        "Modify the partial code below"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D5u-hVDfjJXZ"
      },
      "source": [
        "def random_mean_ci(X, y, num_tests):\n",
        "    # train_results is a list of train accuracy results for the differrent random splits of the dataset\n",
        "    train_results = []\n",
        "    \n",
        "    # test_results is a list of test accuracy results for the differrent random splits of the dataset\n",
        "    test_results = []\n",
        "    \n",
        "    # Write your code here\n",
        "    \n",
        "    # calculate the train mean and the 95% confidence interval for the list of results\n",
        "    train_mean = np.mean(train_results)\n",
        "    train_ci_low, train_ci_high = stats.t.interval(0.95, len(train_results)-1, loc=train_mean, scale=stats.sem(train_results))\n",
        "    \n",
        "    # calculate the test mean and the 95% confidence interval for the list of results\n",
        "    test_mean = np.mean(test_results)\n",
        "    test_ci_low, test_ci_high = stats.t.interval(0.95, len(test_results)-1, loc=test_mean, scale=stats.sem(test_results))\n",
        "    \n",
        "    # validate return types\n",
        "    assert isinstance(train_mean, float) and isinstance(train_ci_low, float) and isinstance(train_ci_high, float), \"return types\"\n",
        "    assert isinstance(test_mean, float) and isinstance(test_ci_low, float) and isinstance(test_ci_high, float), \"return types\"\n",
        "    \n",
        "    return train_mean, train_ci_low, train_ci_high, test_mean, test_ci_low, test_ci_high"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FrNbYeqqjJXc"
      },
      "source": [
        "### Q1 (e)\n",
        "\n",
        "Use the following code to calculate the mean accuracy and 95% confidence interval over 10 random splits"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oerfu1GljJXd"
      },
      "source": [
        "train_mean10, train_low10, train_high10, test_mean10, test_low10, test_high10 = random_mean_ci(X, y, num_tests = 10)\n",
        "print(\"Train mean accuracy over 10 random splits: {}\".format(train_mean10))\n",
        "print(\"Train confidence interval over 10 random splits: [{}, {}]\".format(train_low10, train_high10))\n",
        "print(\"Test mean accuracy over 10 random splits: {}\".format(test_mean10))\n",
        "print(\"Test confidence interval over 10 random splits: [{}, {}]\".format(test_low10, test_high10))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qWqqzPkGjJXh"
      },
      "source": [
        "[ Write your answer here ]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z86lxUvmjJXi"
      },
      "source": [
        "### Q1 (f)\n",
        "\n",
        "Modify the partial code below"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WLMSd9wgjJXm"
      },
      "source": [
        "def random_cm(X, y, num_tests):\n",
        "    # cm_list is a list of confusion matrices for the different random splits of the dataset\n",
        "    cm_list = []\n",
        "    \n",
        "    # Write your code here\n",
        "    \n",
        "    # sum the confusion matrices and return the combined confusion matrix\n",
        "    combined_cm = np.array(cm_list).sum(axis=0)\n",
        "\n",
        "    # validate return type\n",
        "    assert isinstance(combined_cm, np.ndarray), \"return type\"\n",
        "    \n",
        "    return combined_cm"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e19w779ejJXs"
      },
      "source": [
        "### Q1 (g)\n",
        "\n",
        "Use the following code to produce a confusion matrix for 10 random splits"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xXGSnr-xjJXt"
      },
      "source": [
        "cm10 = random_cm(X, y, num_tests = 10)\n",
        "plot_confusion_matrix(cm10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cXgq9z3KjJXw"
      },
      "source": [
        "[ Write your answer here ]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XTFCjNAjjJXx"
      },
      "source": [
        "## Q2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dhXfPuIpjJXy"
      },
      "source": [
        "### Q2 (a)\n",
        "\n",
        "Modify the partial code below"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gf5uco3VjJXz"
      },
      "source": [
        "def feature_num(X, y):\n",
        "    # result_list is a list of tuples (num_features, train_accuracy, test_accuracy)\n",
        "    # where numFeatures is the number of words used as features\n",
        "    result_list = []\n",
        "    \n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "    \n",
        "    for p in [0.1, 0.2, 0.4, 0.6, 0.8, 1.0]:\n",
        "        subset_size = int(p*X.shape[1])\n",
        "        X_train_subset = X_train.iloc[:, 0:subset_size]\n",
        "        X_test_subset = X_test.iloc[:, 0:subset_size]\n",
        "        # Write your code here to calculate train_accuracy and test_accuracy for the current subset of features\n",
        "        # ......\n",
        "        train_accuracy = None\n",
        "        test_accuracy = None\n",
        "        # add to result_list\n",
        "        result_list.append((p, train_accuracy, test_accuracy))\n",
        "        \n",
        "    # Make a dataframe of the results\n",
        "    result_df = pd.DataFrame(result_list, columns=[\"num_features\", \"train_accuracy\", \"test_accuracy\"])\n",
        "    \n",
        "    # validate return type\n",
        "    assert isinstance(result_df, pd.DataFrame), \"return type\"\n",
        "    \n",
        "    return result_df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X7hNvwoljJX6"
      },
      "source": [
        "### Q2 (b)\n",
        "\n",
        "Use the following code to plot the train and test accuracy for the different feature sets sizes"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ehilq_hyjJX7"
      },
      "source": [
        "feature_num_df = feature_num(X, y)\n",
        "feature_num_df.plot(x=\"num_features\", y=[\"train_accuracy\", \"test_accuracy\"])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XFk_ess_jJYC"
      },
      "source": [
        "[ Write your answer here ]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a0ZftHo6jJYD"
      },
      "source": [
        "## Q3"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X0e1QSDsjJYE"
      },
      "source": [
        "### Q3 (a)\n",
        "\n",
        "Modify the partial code below"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F84NTWoHjJYI"
      },
      "source": [
        "def hyperparameter(X, y):\n",
        "    # result_list is a list of tuples (num_features, train_accuracy, test_accuracy)\n",
        "    # where numFeatures is the number of words used as features\n",
        "    result_list = []\n",
        "    \n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "    \n",
        "    for param in [0.001, 0.01, 0.1, 1, 10, 100, 1000]:\n",
        "        # Write your code here to calculate train_accuracy and test_accuracy for the current parameter value\n",
        "        # ......\n",
        "        train_accuracy = None\n",
        "        test_accuracy = None\n",
        "        \n",
        "        # add to result_list\n",
        "        result_list.append((param, train_accuracy, test_accuracy))\n",
        "        \n",
        "    # Make a dataframe of the results\n",
        "    result_df = pd.DataFrame(result_list, columns=[\"param\", \"train_accuracy\", \"test_accuracy\"])\n",
        "    \n",
        "    # validate return type\n",
        "    assert isinstance(result_df, pd.DataFrame), \"return type\"\n",
        "    \n",
        "    return result_df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Id6-n2ZBjJYM"
      },
      "source": [
        "### Q3 (b)\n",
        "\n",
        "Use the following code to plot the train and test accuracy for the different the parameters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "phBkJ5mvjJYN"
      },
      "source": [
        "param_df = hyperparameter(X, y)\n",
        "param_df.plot(x=\"param\", y=[\"train_accuracy\", \"test_accuracy\"], logx=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ngW4KMVLjJYP"
      },
      "source": [
        "[ Write your answer here ]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NPsJTYu_jJYR"
      },
      "source": [
        "## Q4"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aK7nTn82jJYS"
      },
      "source": [
        "### Q4 (a)\n",
        "\n",
        "Modify the partial code below"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8LB6Z1sXjJYT"
      },
      "source": [
        "def tf_improved_data(file_list, num_words = 1000):\n",
        "    # Put your code here\n",
        "    # Make sure you update the variable features and targets below\n",
        "    \n",
        "    X = None\n",
        "    y = None\n",
        "    \n",
        "    # validate return types\n",
        "    assert isinstance(X, pd.DataFrame) and isinstance(y, list), \"return types\"\n",
        "    \n",
        "    return X, y"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C1Ws8ZgrjJYV"
      },
      "source": [
        "### Q4 (b)\n",
        "\n",
        "Use the following code to calculate the mean accuracy and 95% confidence interval over multiple random splits"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A9c3b_RwjJYW"
      },
      "source": [
        "X_tf, y_tf = tf_improved_data(all_files)\n",
        "train_mean10, train_low10, train_high10, test_mean10, test_low10, test_high10 = random_mean_ci(X_tf, y_tf, num_tests = 10)\n",
        "print(\"Train mean accuracy over 10 random splits: {}\".format(train_mean10))\n",
        "print(\"Train confidence interval over 10 random splits: [{}, {}]\".format(train_low10, train_high10))\n",
        "print(\"Test mean accuracy over 10 random splits: {}\".format(test_mean10))\n",
        "print(\"Test confidence interval over 10 random splits: [{}, {}]\".format(test_low10, test_high10))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KKgIMSPRjJYc"
      },
      "source": [
        "[ Write your answer here ]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zAfgSoqjjJYd"
      },
      "source": [
        "## Q5"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2tV_h9kGjJYg"
      },
      "source": [
        "### Q5 (a)\n",
        "\n",
        "Modify the partial code below"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pV7Gad5SjJYh"
      },
      "source": [
        "def nb_random_mean_ci(X, y, num_tests):\n",
        "    # train_results is a list of train accuracy results for the differrent random splits of the dataset\n",
        "    train_results = []\n",
        "    \n",
        "    # test_results is a list of test accuracy results for the differrent random splits of the dataset\n",
        "    test_results = []\n",
        "    \n",
        "    # Write your code here\n",
        "    \n",
        "    # calculate the train mean and the 95% confidence interval for the list of results\n",
        "    train_mean = np.mean(train_results)\n",
        "    train_ci_low, train_ci_high = stats.t.interval(0.95, len(train_results)-1, loc=train_mean, scale=stats.sem(train_results))\n",
        "    \n",
        "    # calculate the test mean and the 95% confidence interval for the list of results\n",
        "    test_mean = np.mean(test_results)\n",
        "    test_ci_low, test_ci_high = stats.t.interval(0.95, len(test_results)-1, loc=test_mean, scale=stats.sem(test_results))\n",
        "    \n",
        "    # validate return types\n",
        "    assert isinstance(train_mean, float) and isinstance(train_ci_low, float) and isinstance(train_ci_high, float), \"return types\"\n",
        "    assert isinstance(test_mean, float) and isinstance(test_ci_low, float) and isinstance(test_ci_high, float), \"return types\"\n",
        "    \n",
        "    return train_mean, train_ci_low, train_ci_high, test_mean, test_ci_low, test_ci_high"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E9-OzAIUjJYj"
      },
      "source": [
        "### Q5 (b)\n",
        "\n",
        "Use the following code to calculate the mean accuracy and 95% confidence interval over multiple random splits"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l9Me9H6HjJYk"
      },
      "source": [
        "train_mean10, train_low10, train_high10, test_mean10, test_low10, test_high10 = nb_random_mean_ci(X, y, num_tests = 10)\n",
        "print(\"Train mean accuracy over 10 random splits: {}\".format(train_mean10))\n",
        "print(\"Train confidence interval over 10 random splits: [{}, {}]\".format(train_low10, train_high10))\n",
        "print(\"Test mean accuracy over 10 random splits: {}\".format(test_mean10))\n",
        "print(\"Test confidence interval over 10 random splits: [{}, {}]\".format(test_low10, test_high10))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P99mWvAbjJYp"
      },
      "source": [
        "[ Write your answer here ]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4CPzZDj-jJYq"
      },
      "source": [
        "## Q6"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tWL6L_OUjJYu"
      },
      "source": [
        "### Q6 (a)\n",
        "\n",
        "Modify the partial code below"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2b7UGAt9jJYv"
      },
      "source": [
        "def binary_med_data(file_list, num_words = 1000):\n",
        "    # Put your code here\n",
        "    # Make sure you update the variable features and targets below\n",
        "    \n",
        "    X = None\n",
        "    y = None\n",
        "\n",
        "    #Please remember to put index for your dataframe as the file name\n",
        "    #For example: pd.DataFrame(data, index=[str(f) for f in file_list],columns=[])\n",
        "\n",
        "    # validate return types\n",
        "    assert isinstance(X, pd.DataFrame) and isinstance(y, list), \"return types\"\n",
        "    \n",
        "    return X, y"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7Z3uKy8WjJYx"
      },
      "source": [
        "### Q6 (b)\n",
        "\n",
        "Use the following code to calculate the mean accuracy and 95% confidence interval over multiple random splits"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D3Dfy0uIjJYy"
      },
      "source": [
        "X, y = binary_med_data(all_files)\n",
        "train_mean10, train_low10, train_high10, test_mean10, test_low10, test_high10 = random_mean_ci(X, y, num_tests = 10)\n",
        "print(\"Train mean accuracy over 10 random splits: {}\".format(train_mean10))\n",
        "print(\"Train confidence interval over 10 random splits: [{}, {}]\".format(train_low10, train_high10))\n",
        "print(\"Test mean accuracy over 10 random splits: {}\".format(test_mean10))\n",
        "print(\"Test confidence interval over 10 random splits: [{}, {}]\".format(test_low10, test_high10))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EueLolt1jJY0"
      },
      "source": [
        "[ Write your answer here ]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kKTrbXOQ6BiE"
      },
      "source": [
        "#Q7"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xfKToZQOqnI-"
      },
      "source": [
        "##Q7(a)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IE2xOCog6GjK"
      },
      "source": [
        "use the following code cell to implement your feature encoding"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eA5_qprNfEsw"
      },
      "source": [
        "def data_q7(file_list, num_words = 1000):\n",
        "  X = None\n",
        "  y = None\n",
        "  \n",
        "  #Please remember to put index for your dataframe as the file name\n",
        "  #For example: pd.DataFrame(data, index=[str(f) for f in file_list],columns=[])\n",
        "  \n",
        "  # validate return types\n",
        "  assert isinstance(X, pd.DataFrame) and isinstance(y, list), \"return types\"\n",
        "  \n",
        "  return X, y"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q95THgZEqqBv"
      },
      "source": [
        "##Q7(b)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M09YbFlKpsyD"
      },
      "source": [
        "Use the following code cell to implement your model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ybxiy9YDjJY1"
      },
      "source": [
        "def build_model_q7():\n",
        "  #write your code here, define your model\n",
        "  MODELQ7=None\n",
        "  return MODELQ7"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vlo2CajIpxXE"
      },
      "source": [
        "Code for evaluating p at k "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rJNEhP0A6u0E"
      },
      "source": [
        "def calculate_average_precision_at_k(MODELQ7, all_files, training_files, testing_files,k=None):\n",
        "  \n",
        "  training_files = [str(f) for f in open(training_files,mode='r').read().splitlines()]\n",
        "  testing_files = [str(f) for f in open(testing_files,mode='r').read().splitlines()]\n",
        "  if k is None:\n",
        "    k=len(testing_files)\n",
        "\n",
        "  X, y = data_q7(all_files) \n",
        "  X['gt'] = y\n",
        "  training = X.loc[training_files]\n",
        "  X_train = training.loc[:,training.columns!='gt']\n",
        "  y_train = training['gt'].values\n",
        "\n",
        "  testing = X.loc[testing_files]\n",
        "  X_test = testing.loc[:,testing.columns!='gt']\n",
        "  y_test = testing['gt'].values\n",
        "\n",
        "  MODELQ7.fit(X_train,y_train)\n",
        "  y_pred = MODELQ7.predict(X_test)\n",
        "  y_pred_prob = MODELQ7.predict_proba(X_test)\n",
        "  confidences = np.max(y_pred_prob,axis=1)\n",
        "  \n",
        "  p_at_k = []\n",
        "  rel_at_k = []\n",
        "  confidence_order = np.argsort(confidences)\n",
        "  for i in range(1,k+1):\n",
        "    top_confidence = confidence_order[-i:]\n",
        "    pred_top_i = y_pred[top_confidence]\n",
        "    gt_top_i = np.array(y_test)[top_confidence]\n",
        "    p_at_i = np.sum(pred_top_i == gt_top_i)/i\n",
        "    rel_at_i = (pred_top_i[0] == gt_top_i[0])\n",
        "    p_at_k.append(p_at_i)\n",
        "    rel_at_k.append(rel_at_i)\n",
        "  print('average precision at {} is {}'.format(k,np.dot(p_at_k,rel_at_k)/k))\n",
        "  return np.dot(p_at_k,rel_at_k)/k"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zoyJdBWUYqBJ"
      },
      "source": [
        "# Example usage:\n",
        "#m = calculate_average_precision_at_k_baseline(LogisticRegression(C=1), all_files, 'training_files_Q7.txt', 'testing_files_Q7.txt')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jOGT3Kh4qutV"
      },
      "source": [
        "##Q7(c)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ropspYfMsb4y"
      },
      "source": [
        "Please write your answers here"
      ]
    }
  ]
}